{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK Biobank Research Analysis Platform - Basic data extraction\n",
    "This notebook is mainly based two notebooks from DNAnexus:\n",
    "https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb\n",
    "https://github.com/dnanexus/UKB_RAP/blob/main/proteomics/0_extract_phenotype_protein_data.ipynb\n",
    "\n",
    "### This notebook consists of three sections:\n",
    "### Step 1: Extract clinical phenotype data from dataset['participant'] \n",
    "### Step 2: Extract Olink proteomics from dataset['olink_instance_0']\n",
    "### Step 3: Extract operation records from dataset['hesin_oper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/spark/python/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "# dxpy allows python to interact with the platform storage\n",
    "# Note: This notebook is using spark since the size of the dataset we're extracting\n",
    "# (i.e. the number of fields) is too large for a single node instance.\n",
    "import dxpy\n",
    "import dxdata\n",
    "\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "# Spark initialization (Done only once; do not rerun this cell unless you select Kernel -> Restart kernel).\n",
    "# Need to adjust this buffer otherwise will get an error in toPandas() call\n",
    "conf = pyspark.SparkConf().set(\"spark.kryoserializer.buffer.max\", \"1024\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normal spark initialization (Done only once; do not rerun this cell unless you select Kernel -> Restart kernel).\n",
    "# sc = pyspark.SparkContext()\n",
    "# spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.41.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dxdata.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# silence warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Re-enable warnings after your code if you want to see warnings again in subsequent cells\n",
    "# warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the option to None to display all rows, or set a specific number\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Reset display options to default\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically discover dispensed database name and dataset id\n",
    "dispensed_database = dxpy.find_one_data_object(\n",
    "    classname='database', \n",
    "    name='app*', \n",
    "    folder='/', \n",
    "    name_mode='glob', \n",
    "    describe=True)\n",
    "dispensed_database_name = dispensed_database['describe']['name']\n",
    "\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename='Dataset', \n",
    "    name='app*.dataset', \n",
    "    folder='/', \n",
    "    name_mode='glob')\n",
    "dispensed_dataset_id = dispensed_dataset['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 'entities' are virtual tables linked to one another.\n",
    "\n",
    "The main entity is 'participant' and corresponds to most pheno fields. Additional entities correspond to linked health care data.\n",
    "Entities starting with 'hesin' are for hospital records; entities starting with 'gp' are for GP records, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Entity \"participant\">,\n",
       " <Entity \"covid19_result_england\">,\n",
       " <Entity \"covid19_result_scotland\">,\n",
       " <Entity \"covid19_result_wales\">,\n",
       " <Entity \"gp_clinical\">,\n",
       " <Entity \"gp_scripts\">,\n",
       " <Entity \"gp_registrations\">,\n",
       " <Entity \"hesin\">,\n",
       " <Entity \"hesin_diag\">,\n",
       " <Entity \"hesin_oper\">,\n",
       " <Entity \"hesin_critical\">,\n",
       " <Entity \"hesin_maternity\">,\n",
       " <Entity \"hesin_delivery\">,\n",
       " <Entity \"hesin_psych\">,\n",
       " <Entity \"death\">,\n",
       " <Entity \"death_cause\">,\n",
       " <Entity \"omop_death\">,\n",
       " <Entity \"omop_device_exposure\">,\n",
       " <Entity \"omop_note\">,\n",
       " <Entity \"omop_observation\">,\n",
       " <Entity \"omop_drug_exposure\">,\n",
       " <Entity \"omop_observation_period\">,\n",
       " <Entity \"omop_person\">,\n",
       " <Entity \"omop_procedure_occurrence\">,\n",
       " <Entity \"omop_specimen\">,\n",
       " <Entity \"omop_visit_detail\">,\n",
       " <Entity \"omop_visit_occurrence\">,\n",
       " <Entity \"omop_dose_era\">,\n",
       " <Entity \"omop_drug_era\">,\n",
       " <Entity \"omop_condition_era\">,\n",
       " <Entity \"omop_condition_occurrence\">,\n",
       " <Entity \"omop_measurement\">,\n",
       " <Entity \"olink_instance_0\">,\n",
       " <Entity \"olink_instance_2\">,\n",
       " <Entity \"olink_instance_3\">]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Accessing the main 'participant' entity\n",
    "The extraction code follows some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant = dataset['participant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing field names, given UKB showcase field id, instance id, and array id\n",
    "\n",
    "For the main participant data, the Platform uses field names with the following convention:\n",
    "\n",
    "|Type of field|Syntax for field name|Example|\n",
    "|:------------|---------------------|-------|\n",
    "|Neither instanced nor arrayed|`p<FIELD-ID>`|`p31`|\n",
    "|Instanced but not arrayed|`p<FIELD-ID>_i<INSTANCE-ID>`|`p40005_i0`|\n",
    "|Arrayed but not instanced|`p<FIELD-ID>_a<ARRAY-ID>`|`p41262_a0`|\n",
    "|Instanced and arrayed|`p<FIELD-ID>_i<INSTANCE-ID>_a<ARRAY-ID>`|`p93_i0_a0`|\n",
    "\n",
    "Lastly, the participant id field itself (EID) is named `eid`\n",
    "\n",
    "If you know exactly the field names you want to work with, put them in a string array (we will see later how to use that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example\n",
    "# field_names = ['eid', 'p31', 'p21022', 'p40005_i0', 'p93_i0_a0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking up fields, given UKB showcase field id\n",
    "\n",
    "If you know the field id but you are not sure if it is instanced or arrayed, and want to grab all instances/arrays (if any), use these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns all field objects for a given UKB showcase field id\n",
    "\n",
    "def fields_for_id(field_id):\n",
    "    from distutils.version import LooseVersion\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "# Returns all field names for a given UKB showcase field id\n",
    "\n",
    "def field_names_for_id(field_id):\n",
    "    return [f.name for f in fields_for_id(field_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p31']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Participant sex\n",
    "field_names_for_id('31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p21003_i0', 'p21003_i1', 'p21003_i2', 'p21003_i3']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Age when attending assessment centre has multiple instances (visits) \n",
    "field_names_for_id('21003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p21001_i0', 'p21001_i1', 'p21001_i2', 'p21001_i3']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_names_for_id('21001') # BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p6150_i0', 'p6150_i1', 'p6150_i2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_names_for_id('6150') # having had a heart attack diagnosed by a doctor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p20002_i0_a0',\n",
       " 'p20002_i0_a1',\n",
       " 'p20002_i0_a2',\n",
       " 'p20002_i0_a3',\n",
       " 'p20002_i0_a4',\n",
       " 'p20002_i0_a5',\n",
       " 'p20002_i0_a6',\n",
       " 'p20002_i0_a7',\n",
       " 'p20002_i0_a8',\n",
       " 'p20002_i0_a9',\n",
       " 'p20002_i0_a10',\n",
       " 'p20002_i0_a11',\n",
       " 'p20002_i0_a12',\n",
       " 'p20002_i0_a13',\n",
       " 'p20002_i0_a14',\n",
       " 'p20002_i0_a15',\n",
       " 'p20002_i0_a16',\n",
       " 'p20002_i0_a17',\n",
       " 'p20002_i0_a18',\n",
       " 'p20002_i0_a19',\n",
       " 'p20002_i0_a20',\n",
       " 'p20002_i0_a21',\n",
       " 'p20002_i0_a22',\n",
       " 'p20002_i0_a23',\n",
       " 'p20002_i0_a24',\n",
       " 'p20002_i0_a25',\n",
       " 'p20002_i0_a26',\n",
       " 'p20002_i0_a27',\n",
       " 'p20002_i0_a28',\n",
       " 'p20002_i0_a29',\n",
       " 'p20002_i0_a30',\n",
       " 'p20002_i0_a31',\n",
       " 'p20002_i0_a32',\n",
       " 'p20002_i0_a33',\n",
       " 'p20002_i1_a0',\n",
       " 'p20002_i1_a1',\n",
       " 'p20002_i1_a2',\n",
       " 'p20002_i1_a3',\n",
       " 'p20002_i1_a4',\n",
       " 'p20002_i1_a5',\n",
       " 'p20002_i1_a6',\n",
       " 'p20002_i1_a7',\n",
       " 'p20002_i1_a8',\n",
       " 'p20002_i1_a9',\n",
       " 'p20002_i1_a10',\n",
       " 'p20002_i1_a11',\n",
       " 'p20002_i1_a12',\n",
       " 'p20002_i1_a13',\n",
       " 'p20002_i1_a14',\n",
       " 'p20002_i1_a15',\n",
       " 'p20002_i1_a16',\n",
       " 'p20002_i1_a17',\n",
       " 'p20002_i1_a18',\n",
       " 'p20002_i1_a19',\n",
       " 'p20002_i1_a20',\n",
       " 'p20002_i1_a21',\n",
       " 'p20002_i1_a22',\n",
       " 'p20002_i1_a23',\n",
       " 'p20002_i1_a24',\n",
       " 'p20002_i1_a25',\n",
       " 'p20002_i1_a26',\n",
       " 'p20002_i1_a27',\n",
       " 'p20002_i1_a28',\n",
       " 'p20002_i1_a29',\n",
       " 'p20002_i1_a30',\n",
       " 'p20002_i1_a31',\n",
       " 'p20002_i1_a32',\n",
       " 'p20002_i1_a33',\n",
       " 'p20002_i2_a0',\n",
       " 'p20002_i2_a1',\n",
       " 'p20002_i2_a2',\n",
       " 'p20002_i2_a3',\n",
       " 'p20002_i2_a4',\n",
       " 'p20002_i2_a5',\n",
       " 'p20002_i2_a6',\n",
       " 'p20002_i2_a7',\n",
       " 'p20002_i2_a8',\n",
       " 'p20002_i2_a9',\n",
       " 'p20002_i2_a10',\n",
       " 'p20002_i2_a11',\n",
       " 'p20002_i2_a12',\n",
       " 'p20002_i2_a13',\n",
       " 'p20002_i2_a14',\n",
       " 'p20002_i2_a15',\n",
       " 'p20002_i2_a16',\n",
       " 'p20002_i2_a17',\n",
       " 'p20002_i2_a18',\n",
       " 'p20002_i2_a19',\n",
       " 'p20002_i2_a20',\n",
       " 'p20002_i2_a21',\n",
       " 'p20002_i2_a22',\n",
       " 'p20002_i2_a23',\n",
       " 'p20002_i2_a24',\n",
       " 'p20002_i2_a25',\n",
       " 'p20002_i2_a26',\n",
       " 'p20002_i2_a27',\n",
       " 'p20002_i2_a28',\n",
       " 'p20002_i2_a29',\n",
       " 'p20002_i2_a30',\n",
       " 'p20002_i2_a31',\n",
       " 'p20002_i2_a32',\n",
       " 'p20002_i2_a33',\n",
       " 'p20002_i3_a0',\n",
       " 'p20002_i3_a1',\n",
       " 'p20002_i3_a2',\n",
       " 'p20002_i3_a3',\n",
       " 'p20002_i3_a4',\n",
       " 'p20002_i3_a5',\n",
       " 'p20002_i3_a6',\n",
       " 'p20002_i3_a7',\n",
       " 'p20002_i3_a8',\n",
       " 'p20002_i3_a9',\n",
       " 'p20002_i3_a10',\n",
       " 'p20002_i3_a11',\n",
       " 'p20002_i3_a12',\n",
       " 'p20002_i3_a13',\n",
       " 'p20002_i3_a14',\n",
       " 'p20002_i3_a15',\n",
       " 'p20002_i3_a16',\n",
       " 'p20002_i3_a17',\n",
       " 'p20002_i3_a18',\n",
       " 'p20002_i3_a19',\n",
       " 'p20002_i3_a20',\n",
       " 'p20002_i3_a21',\n",
       " 'p20002_i3_a22',\n",
       " 'p20002_i3_a23',\n",
       " 'p20002_i3_a24',\n",
       " 'p20002_i3_a25',\n",
       " 'p20002_i3_a26',\n",
       " 'p20002_i3_a27',\n",
       " 'p20002_i3_a28',\n",
       " 'p20002_i3_a29',\n",
       " 'p20002_i3_a30',\n",
       " 'p20002_i3_a31',\n",
       " 'p20002_i3_a32',\n",
       " 'p20002_i3_a33']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_names_for_id('20002') # non-cancer illnesses that self-reported as heart attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p20107_i0', 'p20107_i1', 'p20107_i2', 'p20107_i3']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_names_for_id('20107') # illness of father"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p20003_i0_a0',\n",
       " 'p20003_i0_a1',\n",
       " 'p20003_i0_a2',\n",
       " 'p20003_i0_a3',\n",
       " 'p20003_i0_a4',\n",
       " 'p20003_i0_a5',\n",
       " 'p20003_i0_a6',\n",
       " 'p20003_i0_a7',\n",
       " 'p20003_i0_a8',\n",
       " 'p20003_i0_a9',\n",
       " 'p20003_i0_a10',\n",
       " 'p20003_i0_a11',\n",
       " 'p20003_i0_a12',\n",
       " 'p20003_i0_a13',\n",
       " 'p20003_i0_a14',\n",
       " 'p20003_i0_a15',\n",
       " 'p20003_i0_a16',\n",
       " 'p20003_i0_a17',\n",
       " 'p20003_i0_a18',\n",
       " 'p20003_i0_a19',\n",
       " 'p20003_i0_a20',\n",
       " 'p20003_i0_a21',\n",
       " 'p20003_i0_a22',\n",
       " 'p20003_i0_a23',\n",
       " 'p20003_i0_a24',\n",
       " 'p20003_i0_a25',\n",
       " 'p20003_i0_a26',\n",
       " 'p20003_i0_a27',\n",
       " 'p20003_i0_a28',\n",
       " 'p20003_i0_a29',\n",
       " 'p20003_i0_a30',\n",
       " 'p20003_i0_a31',\n",
       " 'p20003_i0_a32',\n",
       " 'p20003_i0_a33',\n",
       " 'p20003_i0_a34',\n",
       " 'p20003_i0_a35',\n",
       " 'p20003_i0_a36',\n",
       " 'p20003_i0_a37',\n",
       " 'p20003_i0_a38',\n",
       " 'p20003_i0_a39',\n",
       " 'p20003_i0_a40',\n",
       " 'p20003_i0_a41',\n",
       " 'p20003_i0_a42',\n",
       " 'p20003_i0_a43',\n",
       " 'p20003_i0_a44',\n",
       " 'p20003_i0_a45',\n",
       " 'p20003_i0_a46',\n",
       " 'p20003_i0_a47',\n",
       " 'p20003_i1_a0',\n",
       " 'p20003_i1_a1',\n",
       " 'p20003_i1_a2',\n",
       " 'p20003_i1_a3',\n",
       " 'p20003_i1_a4',\n",
       " 'p20003_i1_a5',\n",
       " 'p20003_i1_a6',\n",
       " 'p20003_i1_a7',\n",
       " 'p20003_i1_a8',\n",
       " 'p20003_i1_a9',\n",
       " 'p20003_i1_a10',\n",
       " 'p20003_i1_a11',\n",
       " 'p20003_i1_a12',\n",
       " 'p20003_i1_a13',\n",
       " 'p20003_i1_a14',\n",
       " 'p20003_i1_a15',\n",
       " 'p20003_i1_a16',\n",
       " 'p20003_i1_a17',\n",
       " 'p20003_i1_a18',\n",
       " 'p20003_i1_a19',\n",
       " 'p20003_i1_a20',\n",
       " 'p20003_i1_a21',\n",
       " 'p20003_i1_a22',\n",
       " 'p20003_i1_a23',\n",
       " 'p20003_i1_a24',\n",
       " 'p20003_i1_a25',\n",
       " 'p20003_i1_a26',\n",
       " 'p20003_i1_a27',\n",
       " 'p20003_i1_a28',\n",
       " 'p20003_i1_a29',\n",
       " 'p20003_i1_a30',\n",
       " 'p20003_i1_a31',\n",
       " 'p20003_i1_a32',\n",
       " 'p20003_i1_a33',\n",
       " 'p20003_i1_a34',\n",
       " 'p20003_i1_a35',\n",
       " 'p20003_i1_a36',\n",
       " 'p20003_i1_a37',\n",
       " 'p20003_i1_a38',\n",
       " 'p20003_i1_a39',\n",
       " 'p20003_i1_a40',\n",
       " 'p20003_i1_a41',\n",
       " 'p20003_i1_a42',\n",
       " 'p20003_i1_a43',\n",
       " 'p20003_i1_a44',\n",
       " 'p20003_i1_a45',\n",
       " 'p20003_i1_a46',\n",
       " 'p20003_i1_a47',\n",
       " 'p20003_i2_a0',\n",
       " 'p20003_i2_a1',\n",
       " 'p20003_i2_a2',\n",
       " 'p20003_i2_a3',\n",
       " 'p20003_i2_a4',\n",
       " 'p20003_i2_a5',\n",
       " 'p20003_i2_a6',\n",
       " 'p20003_i2_a7',\n",
       " 'p20003_i2_a8',\n",
       " 'p20003_i2_a9',\n",
       " 'p20003_i2_a10',\n",
       " 'p20003_i2_a11',\n",
       " 'p20003_i2_a12',\n",
       " 'p20003_i2_a13',\n",
       " 'p20003_i2_a14',\n",
       " 'p20003_i2_a15',\n",
       " 'p20003_i2_a16',\n",
       " 'p20003_i2_a17',\n",
       " 'p20003_i2_a18',\n",
       " 'p20003_i2_a19',\n",
       " 'p20003_i2_a20',\n",
       " 'p20003_i2_a21',\n",
       " 'p20003_i2_a22',\n",
       " 'p20003_i2_a23',\n",
       " 'p20003_i2_a24',\n",
       " 'p20003_i2_a25',\n",
       " 'p20003_i2_a26',\n",
       " 'p20003_i2_a27',\n",
       " 'p20003_i2_a28',\n",
       " 'p20003_i2_a29',\n",
       " 'p20003_i2_a30',\n",
       " 'p20003_i2_a31',\n",
       " 'p20003_i2_a32',\n",
       " 'p20003_i2_a33',\n",
       " 'p20003_i2_a34',\n",
       " 'p20003_i2_a35',\n",
       " 'p20003_i2_a36',\n",
       " 'p20003_i2_a37',\n",
       " 'p20003_i2_a38',\n",
       " 'p20003_i2_a39',\n",
       " 'p20003_i2_a40',\n",
       " 'p20003_i2_a41',\n",
       " 'p20003_i2_a42',\n",
       " 'p20003_i2_a43',\n",
       " 'p20003_i2_a44',\n",
       " 'p20003_i2_a45',\n",
       " 'p20003_i2_a46',\n",
       " 'p20003_i2_a47',\n",
       " 'p20003_i3_a0',\n",
       " 'p20003_i3_a1',\n",
       " 'p20003_i3_a2',\n",
       " 'p20003_i3_a3',\n",
       " 'p20003_i3_a4',\n",
       " 'p20003_i3_a5',\n",
       " 'p20003_i3_a6',\n",
       " 'p20003_i3_a7',\n",
       " 'p20003_i3_a8',\n",
       " 'p20003_i3_a9',\n",
       " 'p20003_i3_a10',\n",
       " 'p20003_i3_a11',\n",
       " 'p20003_i3_a12',\n",
       " 'p20003_i3_a13',\n",
       " 'p20003_i3_a14',\n",
       " 'p20003_i3_a15',\n",
       " 'p20003_i3_a16',\n",
       " 'p20003_i3_a17',\n",
       " 'p20003_i3_a18',\n",
       " 'p20003_i3_a19',\n",
       " 'p20003_i3_a20',\n",
       " 'p20003_i3_a21',\n",
       " 'p20003_i3_a22',\n",
       " 'p20003_i3_a23',\n",
       " 'p20003_i3_a24',\n",
       " 'p20003_i3_a25',\n",
       " 'p20003_i3_a26',\n",
       " 'p20003_i3_a27',\n",
       " 'p20003_i3_a28',\n",
       " 'p20003_i3_a29',\n",
       " 'p20003_i3_a30',\n",
       " 'p20003_i3_a31',\n",
       " 'p20003_i3_a32',\n",
       " 'p20003_i3_a33',\n",
       " 'p20003_i3_a34',\n",
       " 'p20003_i3_a35',\n",
       " 'p20003_i3_a36',\n",
       " 'p20003_i3_a37',\n",
       " 'p20003_i3_a38',\n",
       " 'p20003_i3_a39',\n",
       " 'p20003_i3_a40',\n",
       " 'p20003_i3_a41',\n",
       " 'p20003_i3_a42',\n",
       " 'p20003_i3_a43',\n",
       " 'p20003_i3_a44',\n",
       " 'p20003_i3_a45',\n",
       " 'p20003_i3_a46',\n",
       " 'p20003_i3_a47']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_names_for_id('20003') # medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p191']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_names_for_id('191') # date lost to follow up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain field name for all instances and arrays for each field_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk factors\n",
    "\"We defined risk factors at the first assessment as follows: diabetes diagnosed by a doctor (field #2443), BMI (field #21001), current smoking (field #20116), hypertension, family history of heart disease, and high cholesterol. For hypertension we used an expanded definition including self-reported high blood pressure (either on blood pressure medication, data fields #6177, #6153; systolic blood pressure >140 mm Hg, fields #4080, #93; or diastolic blood pressure >90 mm Hg, data fields #4079, #94). For family history of heart disease, we considered history in any first- degree relative (father, mother, sibling; fields #20107, 20110, and 20111, respectively). For high cholesterol, we considered individuals with self- reported high cholesterol at assessment, as well as diagnoses in the HES/death records (ICD-9 272.0; ICD-10 E78.0). For the analyses of the number of elevated risk factors, we considered diagnosed dia- betes (yes/no), hypertension at assessment (yes/no), BMI >30 kg/m2, smoking at assessment (yes/no), high cholesterol (yes/no), and family history of heart disease (yes/no).\"\n",
    "\n",
    " Am Coll Cardiol. 2018 Oct 16;72(16):1883-1893. \n",
    "https://pubmed.ncbi.nlm.nih.gov/30309464/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p2443_i0',\n",
       " 'p2443_i1',\n",
       " 'p2443_i2',\n",
       " 'p2443_i3',\n",
       " 'p20116_i0',\n",
       " 'p20116_i1',\n",
       " 'p20116_i2',\n",
       " 'p20116_i3',\n",
       " 'p6177_i0',\n",
       " 'p6177_i1',\n",
       " 'p6177_i2',\n",
       " 'p6177_i3',\n",
       " 'p6153_i0',\n",
       " 'p6153_i1',\n",
       " 'p6153_i2',\n",
       " 'p6153_i3',\n",
       " 'p4080_i0_a0',\n",
       " 'p4080_i0_a1',\n",
       " 'p4080_i1_a0',\n",
       " 'p4080_i1_a1',\n",
       " 'p4080_i2_a0',\n",
       " 'p4080_i2_a1',\n",
       " 'p4080_i3_a0',\n",
       " 'p4080_i3_a1',\n",
       " 'p93_i0_a0',\n",
       " 'p93_i0_a1',\n",
       " 'p93_i1_a0',\n",
       " 'p93_i1_a1',\n",
       " 'p93_i2_a0',\n",
       " 'p93_i2_a1',\n",
       " 'p93_i3_a0',\n",
       " 'p93_i3_a1',\n",
       " 'p4079_i0_a0',\n",
       " 'p4079_i0_a1',\n",
       " 'p4079_i1_a0',\n",
       " 'p4079_i1_a1',\n",
       " 'p4079_i2_a0',\n",
       " 'p4079_i2_a1',\n",
       " 'p4079_i3_a0',\n",
       " 'p4079_i3_a1',\n",
       " 'p94_i0_a0',\n",
       " 'p94_i0_a1',\n",
       " 'p94_i1_a0',\n",
       " 'p94_i1_a1',\n",
       " 'p94_i2_a0',\n",
       " 'p94_i2_a1',\n",
       " 'p94_i3_a0',\n",
       " 'p94_i3_a1',\n",
       " 'p20107_i0',\n",
       " 'p20107_i1',\n",
       " 'p20107_i2',\n",
       " 'p20107_i3',\n",
       " 'p20110_i0',\n",
       " 'p20110_i1',\n",
       " 'p20110_i2',\n",
       " 'p20110_i3',\n",
       " 'p20111_i0',\n",
       " 'p20111_i1',\n",
       " 'p20111_i2',\n",
       " 'p20111_i3',\n",
       " 'p30901_i0',\n",
       " 'p30901_i1',\n",
       " 'p30901_i2',\n",
       " 'p30901_i3']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# risk factor + family history + plate id\n",
    "field_ids = ['2443', '20116', '6177', '6153', '4080', '93', '4079', '94', \n",
    "             '20107', '20110', '20111',\n",
    "             '30901']\n",
    "# sum flattens list of lists\n",
    "sum([field_names_for_id(field_id) for field_id in field_ids], []) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking up fields by title keyword\n",
    "\n",
    "If you remember part of the field title, use these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns all field objects for a given title keyword\n",
    "\n",
    "def fields_by_title_keyword(keyword):\n",
    "    from distutils.version import LooseVersion\n",
    "    fields = list(participant.find_fields(lambda f: keyword.lower() in f.title.lower()))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "# Returns all field names for a given title keyword\n",
    "\n",
    "def field_names_by_title_keyword(keyword):\n",
    "    return [f.name for f in fields_by_title_keyword(keyword)]\n",
    "\n",
    "# Returns all field titles for a given title keyword\n",
    "\n",
    "def field_titles_by_title_keyword(keyword):\n",
    "    return [f.title for f in fields_by_title_keyword(keyword)]\n",
    "\n",
    "# Furhter information: https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing fields into a Spark DataFrame\n",
    "\n",
    "The `participant.retrieve_fields` function can be used to construct a Spark DataFrame of the given fields.\n",
    "\n",
    "By default, this retrieves data as encoded by UK Biobank. For example, field p31 (participant sex) will be returned as an integer column with values of 0 and 1. To receive decoded values, supply the `coding_values='replace'` argument.\n",
    "\n",
    "For more information, see [Tips for Retrieving Fields](https://dnanexus.gitbook.io/uk-biobank-rap/getting-started/working-with-ukb-data#tips-for-retrieving-fields) in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract clinical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_names = ['eid', \n",
    "               'p31',  # sex\n",
    "               'p21022',  # age at recruitment\n",
    "               'p21001_i0',  # BMI\n",
    "               'p54_i0',  # UK Biobank assessment centre\n",
    "               'p53_i0',  # Date of attending assessment centre p53_i0\n",
    "               'p40000_i0',  # Date of death\n",
    "               'p191', # date lost to follow-up\n",
    "               'p20003_i0_a0', # medication\n",
    "               'p6150_i0',  # having had a heart attack diagnosed by a doctor \n",
    "               'p20002_i0_a0',  # non-cancer illnesses that self-reported as heart attack \n",
    "               'p20004_i0_a0',  # self-reported operation including PTCA, CABG, or triple heart bypass\n",
    "               'p2443_i0',  # diabetes\n",
    "               'p20116_i0',  # current smoking\n",
    "               'p20160_i0', # ever smoked\n",
    "               'p6177_i0',  # self-reported high blood pressure\n",
    "               'p6153_i0',  # self-reported high blood pressure\n",
    "               'p4080_i0_a0',  # systolic blood pressure \n",
    "               'p93_i0_a0',  # systolic blood pressure \n",
    "               'p4079_i0_a0',  # diastolic blood pressure\n",
    "               'p94_i0_a0', # diastolic blood pressure\n",
    "               'p6177_i0', # Medication for cholesterol, blood pressure or diabetes | Instance 0\n",
    "               'p20107_i0', #illness of father\n",
    "               'p20110_i1', # illness of mother\n",
    "               'p20111_i0', # illness of sibling\n",
    "               'p30901_i0', # olink plateID\n",
    "              ] \\\n",
    "                + field_names_for_id('41270') \\\n",
    "                + field_names_for_id('41280')  # Corrected line\n",
    "\n",
    "# 41270 = ICD10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Further informatiaon on Date of first in-patient diagnosis can be found at https://biobank.ndph.ox.ac.uk/crystal/field.cgi?id=41280:\n",
    "The corresponding ICD-10 diagnosis codes can be found in data-field Field 41270 and the two fields can be linked using the array structure.\n",
    "\n",
    "• CAD definition and risk factors:\n",
    "J Am Coll Cardiol. 2018 Oct 16;72(16):1883-1893. \n",
    "https://pubmed.ncbi.nlm.nih.gov/30309464/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing fields into a Spark DataFrame\n",
    "The participant.retrieve_fields function can be used to construct a Spark DataFrame of the given fields.\n",
    "\n",
    "By default, this retrieves data as encoded by UK Biobank. For example, field p31 (participant sex) will be returned as an integer column with values of 0 and 1. To receive decoded values, supply the coding_values='replace' argument.\n",
    "\n",
    "For more information, see Tips for Retrieving Fields in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grabbing fields into a Spark DataFrame\n",
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See the first five entries as a Spark DataFrame:\n",
    "# df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>p31</th>\n",
       "      <th>p21022</th>\n",
       "      <th>p21001_i0</th>\n",
       "      <th>p54_i0</th>\n",
       "      <th>p53_i0</th>\n",
       "      <th>p40000_i0</th>\n",
       "      <th>p191</th>\n",
       "      <th>p20003_i0_a0</th>\n",
       "      <th>p6150_i0</th>\n",
       "      <th>...</th>\n",
       "      <th>p41280_a249</th>\n",
       "      <th>p41280_a250</th>\n",
       "      <th>p41280_a251</th>\n",
       "      <th>p41280_a252</th>\n",
       "      <th>p41280_a253</th>\n",
       "      <th>p41280_a254</th>\n",
       "      <th>p41280_a255</th>\n",
       "      <th>p41280_a256</th>\n",
       "      <th>p41280_a257</th>\n",
       "      <th>p41280_a258</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2789877</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>22.6630</td>\n",
       "      <td>11004</td>\n",
       "      <td>2008-04-07</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.140868e+09</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3070454</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>25.3439</td>\n",
       "      <td>11009</td>\n",
       "      <td>2008-03-14</td>\n",
       "      <td>2012-06-03</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2733030</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>24.8491</td>\n",
       "      <td>11016</td>\n",
       "      <td>2009-04-08</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3039657</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>29.3577</td>\n",
       "      <td>10003</td>\n",
       "      <td>2006-03-21</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.140880e+09</td>\n",
       "      <td>[4]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4404117</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>34.1036</td>\n",
       "      <td>11014</td>\n",
       "      <td>2010-06-22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.140917e+09</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       eid  p31  p21022  p21001_i0  p54_i0      p53_i0   p40000_i0  p191  \\\n",
       "0  2789877    1      56    22.6630   11004  2008-04-07        None  None   \n",
       "1  3070454    1      61    25.3439   11009  2008-03-14  2012-06-03  None   \n",
       "2  2733030    0      59    24.8491   11016  2009-04-08        None  None   \n",
       "3  3039657    1      57    29.3577   10003  2006-03-21        None  None   \n",
       "4  4404117    0      49    34.1036   11014  2010-06-22        None  None   \n",
       "\n",
       "   p20003_i0_a0 p6150_i0  ...  p41280_a249  p41280_a250  p41280_a251  \\\n",
       "0  1.140868e+09     [-7]  ...         None         None         None   \n",
       "1           NaN     [-7]  ...         None         None         None   \n",
       "2           NaN     [-7]  ...         None         None         None   \n",
       "3  1.140880e+09      [4]  ...         None         None         None   \n",
       "4  1.140917e+09     [-7]  ...         None         None         None   \n",
       "\n",
       "   p41280_a252  p41280_a253 p41280_a254 p41280_a255  p41280_a256  p41280_a257  \\\n",
       "0         None         None        None        None         None         None   \n",
       "1         None         None        None        None         None         None   \n",
       "2         None         None        None        None         None         None   \n",
       "3         None         None        None        None         None         None   \n",
       "4         None         None        None        None         None         None   \n",
       "\n",
       "   p41280_a258  \n",
       "0         None  \n",
       "1         None  \n",
       "2         None  \n",
       "3         None  \n",
       "4         None  \n",
       "\n",
       "[5 rows x 285 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the first five entries as a Pandas DataFrame:\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if the above looks good, go ahead and convert the entire spark data frame to pandas data frame \n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['eid', 'p31', 'p21022', 'p21001_i0', 'p54_i0', 'p53_i0', 'p40000_i0',\n",
      "       'p191', 'p20003_i0_a0', 'p6150_i0',\n",
      "       ...\n",
      "       'p41280_a249', 'p41280_a250', 'p41280_a251', 'p41280_a252',\n",
      "       'p41280_a253', 'p41280_a254', 'p41280_a255', 'p41280_a256',\n",
      "       'p41280_a257', 'p41280_a258'],\n",
      "      dtype='object', length=285)\n"
     ]
    }
   ],
   "source": [
    "print(pdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the option to None to display all rows, or set a specific number\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Your code to display the Series or DataFrame\n",
    "# For example, if you have a Series named non_na_counts:\n",
    "# pdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Given the information above, re-select the column if needed\n",
    "#field_names = ['p31', 'p21022', 'p54_i0', 'p41202', 'p53_i0', 'p40000_i0']  \\\n",
    "#    + field_names_for_id('41280') \n",
    "#df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset display options to default\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving as TSV file\n",
    "pdf.to_csv('clinical_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Olink proteomics from dataset['olink_instance_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "olink = dataset['olink_instance_0']\n",
    "# olink.fields # to list all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_list = olink.fields\n",
    "type(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def field_names_for_id(field_id):\n",
    "#    return [f.name for f in fields_for_id(field_id)]\n",
    "olink_all_field_names = [f.name for f in olink.fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eid', 'a1bg', 'aamdc', 'aarsd1', 'abca2']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#field_names = ['eid', 'col6a3'] # select like this if you're interested in only a few protein\n",
    "olink_all_field_names[:5] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfo = olink.retrieve_fields(names=olink_all_field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to check\n",
    "# dfo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case of extracting particular proteins only:\n",
    "olink_field_names = ['eid', 'col6a3']\n",
    "dfo = olink.retrieve_fields(names=olink_field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See the first five entries as a Pandas DataFrame:\n",
    "# dfo.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53016"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfo.count() # check rows (53016 individual's data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo = dfo.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53016, 2924)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfo.shape # (53016, 2924)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root] ERROR: KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/cluster/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/cluster/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     chunk_df \u001b[38;5;241m=\u001b[39m dfo_with_id\u001b[38;5;241m.\u001b[39mfilter(dfo_with_id\u001b[38;5;241m.\u001b[39mrow_id \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_index)\u001b[38;5;241m.\u001b[39mlimit(rows_per_chunk)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Convert the chunk to a pandas DataFrame and append to the list\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     chunk_pd_df \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrow_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     chunks_list\u001b[38;5;241m.\u001b[39mappend(chunk_pd_df)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Concatenate all chunks to form the final pandas DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/spark/python/pyspark/sql/pandas/conversion.py:157\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    158\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    160\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/cluster/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Alternative approach\n",
    "# pdfo = dfo.toPandas() is very memory intensive. So instead, we can do this sequentially (if needed).\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = dfo.count()\n",
    "\n",
    "# Number of chunks\n",
    "num_chunks = 10\n",
    "\n",
    "# Calculate the number of rows per chunk. Adding 1 to ensure the last chunk includes all remaining rows\n",
    "rows_per_chunk = (total_rows // num_chunks) + (total_rows % num_chunks > 0)\n",
    "\n",
    "# Initialize an empty list to store each chunk's pandas DataFrame\n",
    "chunks_list = []\n",
    "\n",
    "# Create a column 'row_id' to help in filtering rows for each chunk\n",
    "dfo_with_id = dfo.withColumn(\"row_id\", row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start index for the current chunk\n",
    "    start_index = i * rows_per_chunk\n",
    "    \n",
    "    # End index is not needed as we limit the number of rows fetched\n",
    "    chunk_df = dfo_with_id.filter(dfo_with_id.row_id >= start_index).limit(rows_per_chunk)\n",
    "    \n",
    "    # Convert the chunk to a pandas DataFrame and append to the list\n",
    "    chunk_pd_df = chunk_df.drop(\"row_id\").toPandas()\n",
    "    chunks_list.append(chunk_pd_df)\n",
    "\n",
    "# Concatenate all chunks to form the final pandas DataFrame\n",
    "pdf = pd.concat(chunks_list, ignore_index=True)\n",
    "\n",
    "# Checking the shape of the final DataFrame\n",
    "print(pdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo.iloc[:5, :5] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo.to_csv('olink_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Extract operation records from dataset['hesin_oper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operation = dataset['hesin_oper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Field \"dnx_hesin_oper_id\">,\n",
       " <Field \"dnx_hesin_id\">,\n",
       " <Field \"eid\">,\n",
       " <Field \"ins_index\">,\n",
       " <Field \"preopdur\">,\n",
       " <Field \"arr_index\">,\n",
       " <Field \"level\">,\n",
       " <Field \"opdate\">,\n",
       " <Field \"oper3\">,\n",
       " <Field \"oper3_nb\">,\n",
       " <Field \"oper4\">,\n",
       " <Field \"oper4_nb\">,\n",
       " <Field \"posopdur\">]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operation.fields[1:5] # check\n",
    "operation.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dnx_hesin_oper_id', 'dnx_hesin_id', 'eid', 'ins_index', 'preopdur', 'arr_index', 'level', 'opdate', 'oper3', 'oper3_nb', 'oper4', 'oper4_nb', 'posopdur']\n"
     ]
    }
   ],
   "source": [
    "operation_all_field_names = [f.name for f in operation.fields]\n",
    "print(operation_all_field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ope = operation.retrieve_fields(names=operation_all_field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dnx_hesin_oper_id='4631925-4-0', dnx_hesin_id='4631925-4', eid='4631925', ins_index=4, preopdur=0, arr_index=0, level='1', opdate=datetime.date(2015, 12, 4), oper3=None, oper3_nb=None, oper4='W822', oper4_nb=None, posopdur=0),\n",
       " Row(dnx_hesin_oper_id='3187543-12-8', dnx_hesin_id='3187543-12', eid='3187543', ins_index=12, preopdur=None, arr_index=8, level='2', opdate=datetime.date(2016, 2, 26), oper3=None, oper3_nb=None, oper4='K634', oper4_nb=None, posopdur=None),\n",
       " Row(dnx_hesin_oper_id='5471173-4-14', dnx_hesin_id='5471173-4', eid='5471173', ins_index=4, preopdur=None, arr_index=14, level='2', opdate=datetime.date(2016, 9, 27), oper3=None, oper3_nb=None, oper4='E492', oper4_nb=None, posopdur=None),\n",
       " Row(dnx_hesin_oper_id='2559576-0-2', dnx_hesin_id='2559576-0', eid='2559576', ins_index=0, preopdur=None, arr_index=2, level='2', opdate=datetime.date(2011, 9, 1), oper3=None, oper3_nb=None, oper4='Y819', oper4_nb=None, posopdur=None),\n",
       " Row(dnx_hesin_oper_id='1886597-0-4', dnx_hesin_id='1886597-0', eid='1886597', ins_index=0, preopdur=None, arr_index=4, level='2', opdate=datetime.date(2014, 1, 29), oper3=None, oper3_nb=None, oper4='Y767', oper4_nb=None, posopdur=None)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ope.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnx_hesin_oper_id</th>\n",
       "      <th>dnx_hesin_id</th>\n",
       "      <th>eid</th>\n",
       "      <th>ins_index</th>\n",
       "      <th>preopdur</th>\n",
       "      <th>arr_index</th>\n",
       "      <th>level</th>\n",
       "      <th>opdate</th>\n",
       "      <th>oper3</th>\n",
       "      <th>oper3_nb</th>\n",
       "      <th>oper4</th>\n",
       "      <th>oper4_nb</th>\n",
       "      <th>posopdur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4631925-4-0</td>\n",
       "      <td>4631925-4</td>\n",
       "      <td>4631925</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-12-04</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>W822</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3187543-12-8</td>\n",
       "      <td>3187543-12</td>\n",
       "      <td>3187543</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-26</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>K634</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5471173-4-14</td>\n",
       "      <td>5471173-4</td>\n",
       "      <td>5471173</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-09-27</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E492</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2559576-0-2</td>\n",
       "      <td>2559576-0</td>\n",
       "      <td>2559576</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Y819</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1886597-0-4</td>\n",
       "      <td>1886597-0</td>\n",
       "      <td>1886597</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Y767</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dnx_hesin_oper_id dnx_hesin_id      eid  ins_index  preopdur  arr_index  \\\n",
       "0       4631925-4-0    4631925-4  4631925          4       0.0          0   \n",
       "1      3187543-12-8   3187543-12  3187543         12       NaN          8   \n",
       "2      5471173-4-14    5471173-4  5471173          4       NaN         14   \n",
       "3       2559576-0-2    2559576-0  2559576          0       NaN          2   \n",
       "4       1886597-0-4    1886597-0  1886597          0       NaN          4   \n",
       "\n",
       "  level      opdate oper3 oper3_nb oper4 oper4_nb  posopdur  \n",
       "0     1  2015-12-04  None     None  W822     None       0.0  \n",
       "1     2  2016-02-26  None     None  K634     None       NaN  \n",
       "2     2  2016-09-27  None     None  E492     None       NaN  \n",
       "3     2  2011-09-01  None     None  Y819     None       NaN  \n",
       "4     2  2014-01-29  None     None  Y767     None       NaN  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ope.limit(5).toPandas().head() # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is too memory intensive\n",
    "# pdf_ope = df_ope.dropna(subset=['oper4']).toPandas()\n",
    "# pdf_ope.to_csv('operation_data.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8763355, 13)\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we do this sequentially\n",
    "# from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = df_ope.count()\n",
    "\n",
    "# Number of chunks\n",
    "num_chunks = 10\n",
    "\n",
    "# Calculate the number of rows per chunk. Adding 1 to ensure the last chunk includes all remaining rows\n",
    "rows_per_chunk = (total_rows // num_chunks) + (total_rows % num_chunks > 0)\n",
    "\n",
    "# Initialize an empty list to store each chunk's pandas DataFrame\n",
    "chunks_list = []\n",
    "\n",
    "# Create a column 'row_id' to help in filtering rows for each chunk\n",
    "df_ope_with_id = df_ope.withColumn(\"row_id\", row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start index for the current chunk\n",
    "    start_index = i * rows_per_chunk\n",
    "    \n",
    "    # End index is not needed as we limit the number of rows fetched\n",
    "    chunk_df = df_ope_with_id.filter(df_ope_with_id.row_id >= start_index).limit(rows_per_chunk)\n",
    "    \n",
    "    # Convert the chunk to a pandas DataFrame and append to the list\n",
    "    chunk_pd_df = chunk_df.drop(\"row_id\").toPandas()\n",
    "    chunks_list.append(chunk_pd_df)\n",
    "\n",
    "# Concatenate all chunks to form the final pandas DataFrame\n",
    "pdf_ope = pd.concat(chunks_list, ignore_index=True)\n",
    "\n",
    "# Checking the shape of the final DataFrame\n",
    "print(pdf_ope.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_ope.to_csv('operation_data.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hesin is optional\n",
    "# hesin = dataset['hesin']\n",
    "# hesin_all_field_names = [f.name for f in hesin.fields]\n",
    "# print(hesin_all_field_names)\n",
    "# df_hesin = hesin.retrieve_fields(names=hesin_all_field_names, engine=dxdata.connect())\n",
    "# df_hesin.limit(5).toPandas().head() # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dnx_hesin_diag_id', 'dnx_hesin_id', 'eid', 'ins_index', 'arr_index', 'level', 'diag_icd9', 'diag_icd9_nb', 'diag_icd10', 'diag_icd10_nb']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnx_hesin_diag_id</th>\n",
       "      <th>dnx_hesin_id</th>\n",
       "      <th>eid</th>\n",
       "      <th>ins_index</th>\n",
       "      <th>arr_index</th>\n",
       "      <th>level</th>\n",
       "      <th>diag_icd9</th>\n",
       "      <th>diag_icd9_nb</th>\n",
       "      <th>diag_icd10</th>\n",
       "      <th>diag_icd10_nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4816350-0-0</td>\n",
       "      <td>4816350-0</td>\n",
       "      <td>4816350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>D140</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3835670-9-2</td>\n",
       "      <td>3835670-9</td>\n",
       "      <td>3835670</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Z800</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2693369-4-8</td>\n",
       "      <td>2693369-4</td>\n",
       "      <td>2693369</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N183</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3888853-17-11</td>\n",
       "      <td>3888853-17</td>\n",
       "      <td>3888853</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Z888</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5787855-42-1</td>\n",
       "      <td>5787855-42</td>\n",
       "      <td>5787855</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>M059</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dnx_hesin_diag_id dnx_hesin_id      eid  ins_index  arr_index level  \\\n",
       "0       4816350-0-0    4816350-0  4816350          0          0     1   \n",
       "1       3835670-9-2    3835670-9  3835670          9          2     2   \n",
       "2       2693369-4-8    2693369-4  2693369          4          8     2   \n",
       "3     3888853-17-11   3888853-17  3888853         17         11     2   \n",
       "4      5787855-42-1   5787855-42  5787855         42          1     2   \n",
       "\n",
       "  diag_icd9 diag_icd9_nb diag_icd10 diag_icd10_nb  \n",
       "0      None         None       D140          None  \n",
       "1      None         None       Z800          None  \n",
       "2      None         None       N183          None  \n",
       "3      None         None       Z888          None  \n",
       "4      None         None       M059          None  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrive hesin_diag, which contains ICD10\n",
    "hesin_diag = dataset['hesin_diag']\n",
    "hesin_diag_all_field_names = [f.name for f in hesin_diag.fields]\n",
    "print(hesin_diag_all_field_names)\n",
    "df_hesin_diag = hesin_diag.retrieve_fields(names=hesin_diag_all_field_names, engine=dxdata.connect())\n",
    "df_hesin_diag.limit(5).toPandas().head() # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m pdf_hesin_diag \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(chunks_list, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Checking the shape of the final DataFrame\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_hesin_diag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "File \u001b[0;32m/cluster/spark/python/pyspark/sql/dataframe.py:1659\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1660\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m   1661\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# again, toPandas() is too memory intensive\n",
    "# pdf_hesin_diag = df_hesin_diag.dropna(subset = ['diag_icd10']).toPandas()\n",
    "# Alternatively, we do this sequentially\n",
    "# from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = df_hesin_diag.count()\n",
    "\n",
    "# Number of chunks\n",
    "num_chunks = 10\n",
    "\n",
    "# Calculate the number of rows per chunk. Adding 1 to ensure the last chunk includes all remaining rows\n",
    "rows_per_chunk = (total_rows // num_chunks) + (total_rows % num_chunks > 0)\n",
    "\n",
    "# Initialize an empty list to store each chunk's pandas DataFrame\n",
    "chunks_list = []\n",
    "\n",
    "# Create a column 'row_id' to help in filtering rows for each chunk\n",
    "df_hesin_diag_with_id = df_hesin_diag.withColumn(\"row_id\", row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start index for the current chunk\n",
    "    start_index = i * rows_per_chunk\n",
    "    \n",
    "    # End index is not needed as we limit the number of rows fetched\n",
    "    chunk_df = df_hesin_diag_with_id.filter(df_hesin_diag_with_id.row_id >= start_index).limit(rows_per_chunk)\n",
    "    \n",
    "    # Convert the chunk to a pandas DataFrame and append to the list\n",
    "    chunk_pd_df = chunk_df.drop(\"row_id\").toPandas()\n",
    "    chunks_list.append(chunk_pd_df)\n",
    "\n",
    "# Concatenate all chunks to form the final pandas DataFrame\n",
    "pdf_hesin_diag = pd.concat(chunks_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17494320, 10)\n"
     ]
    }
   ],
   "source": [
    "print(pdf_hesin_diag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_hesin_diag.to_csv('operation_icd10_data.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge df_ope and df_hesin_diag (this is too memory intensive)\n",
    "# pdf_ope_diag = pd.merge(pdf_ope, pdf_hesin_diag, on='eid', how='inner')\n",
    "# pdf_ope_diag.head() # check\n",
    "# pdf_ope_diag.to_csv('operation_data_with_icd10.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# retrive death record\n",
    "hesin = dataset['hesin']\n",
    "hesin_all_field_names = [f.name for f in hesin.fields]\n",
    "print(hesin_all_field_names)\n",
    "df_hesin = hesin.retrieve_fields(names=hesin_all_field_names, engine=dxdata.connect())\n",
    "df_hesin.limit(5).toPandas().head() # check#### Step 4: Extract death record\n",
    "4.1. <Entity \"death\">,\n",
    "4.2 <Entity \"death_cause\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death = dataset['death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Field \"dnx_death_id\">,\n",
       " <Field \"eid\">,\n",
       " <Field \"ins_index\">,\n",
       " <Field \"dsource\">,\n",
       " <Field \"source\">,\n",
       " <Field \"date_of_death\">]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dnx_death_id', 'eid', 'ins_index', 'dsource', 'source', 'date_of_death']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dnx_death_id='4590254-0', eid='4590254', ins_index=0, dsource='E/W', source='1', date_of_death=datetime.date(2009, 11, 25)),\n",
       " Row(dnx_death_id='3989605-0', eid='3989605', ins_index=0, dsource='E/W', source='2', date_of_death=datetime.date(2019, 11, 29)),\n",
       " Row(dnx_death_id='2042144-0', eid='2042144', ins_index=0, dsource='E/W', source='2', date_of_death=datetime.date(2015, 10, 20)),\n",
       " Row(dnx_death_id='5651155-0', eid='5651155', ins_index=0, dsource='SCOT', source='55', date_of_death=datetime.date(2020, 10, 18)),\n",
       " Row(dnx_death_id='3253040-0', eid='3253040', ins_index=0, dsource='E/W', source='2', date_of_death=datetime.date(2016, 10, 14))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death_all_field_names = [f.name for f in death.fields]\n",
    "print(death_all_field_names)\n",
    "df_death = death.retrieve_fields(names=death_all_field_names, engine=dxdata.connect())\n",
    "df_death.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_death = df_death.toPandas() # convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_death.to_csv('death_data.tsv', sep= '\\t', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4.2\n",
    "death_cause = dataset['death_cause']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Field \"dnx_death_cause_id\">, <Field \"dnx_death_id\">, <Field \"eid\">, <Field \"ins_index\">, <Field \"arr_index\">, <Field \"level\">, <Field \"cause_icd10\">]\n"
     ]
    }
   ],
   "source": [
    "print(death_cause.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dnx_death_cause_id', 'dnx_death_id', 'eid', 'ins_index', 'arr_index', 'level', 'cause_icd10']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dnx_death_cause_id='1910421-0-4', dnx_death_id='1910421-0', eid='1910421', ins_index=0, arr_index=4, level=2, cause_icd10='I38'),\n",
       " Row(dnx_death_cause_id='1338497-0-1', dnx_death_id='1338497-0', eid='1338497', ins_index=0, arr_index=1, level=2, cause_icd10='F03'),\n",
       " Row(dnx_death_cause_id='1451607-0-0', dnx_death_id='1451607-0', eid='1451607', ins_index=0, arr_index=0, level=1, cause_icd10='U071'),\n",
       " Row(dnx_death_cause_id='4443217-0-1', dnx_death_id='4443217-0', eid='4443217', ins_index=0, arr_index=1, level=2, cause_icd10='R58'),\n",
       " Row(dnx_death_cause_id='1043906-0-0', dnx_death_id='1043906-0', eid='1043906', ins_index=0, arr_index=0, level=1, cause_icd10='I608')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death_cause_all_field_names = [f.name for f in death_cause.fields]\n",
    "print(death_cause_all_field_names)\n",
    "df_death_cause = death_cause.retrieve_fields(names=death_cause_all_field_names, engine=dxdata.connect())\n",
    "df_death_cause.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_death_cause = df_death_cause.toPandas() # convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_death_cause.to_csv('death_cause_data.tsv', sep= '\\t', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload the resultant files and the current notebook (after saving it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                          file-Gj68840JBPpg9y5FvKQbKzQ7\n",
      "Class                       file\n",
      "Project                     project-Gf7Jk4QJBPpxxxG2x187pyJg\n",
      "Folder                      /data/03.incident_CAD\n",
      "Name                        clinical_data.tsv\n",
      "State                       closing\n",
      "Visibility                  visible\n",
      "Types                       -\n",
      "Properties                  -\n",
      "Tags                        -\n",
      "Outgoing links              -\n",
      "Created                     Tue Apr  2 22:17:52 2024\n",
      "Created by                  satoshi.yoshiji\n",
      " via the job                job-Gj62ZpjJBPpk8zQ88x4V4b0K\n",
      "Last modified               Tue Apr  2 22:17:54 2024\n",
      "Media type                  \n",
      "archivalState               \"live\"\n",
      "cloudAccount                \"cloudaccount-dnanexus\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "dx upload clinical_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload olink_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload operation_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload operation_icd10_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload death_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload death_cause_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload ukbrap_extract_clinical_olink_ope_data.ipynb --dest UKB:/data/03.incident_CAD/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Below are miscellaneous codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if you want to combine data frame, use pandas's merge \n",
    "combined = pd.merge(pdf, df_final, on = 'eid', how= 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering spark dataframes\n",
    "\n",
    "Spark dataframes can be filtered using the syntax: `df.filter(expression)`\n",
    "\n",
    "The expression can be either :\n",
    "\n",
    "* a string expression, built using SQL field names (e.g. `p31`) and SQL operators (e.g. `=`, `NOT`, `OR`, `AND`)\n",
    "  * example: `'(p21022 >= 50) AND (p31 = 0)'`\n",
    "  \n",
    "\n",
    "* a Python expression, built using Python object fields (e.g. `df.p31`) and Python operators (e.g. `==`, `!`, `|`, `&`)\n",
    "  * example: `(df.p21022 >= 50) & (df.p31 == 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Participants above 50 years old and female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL syntax\n",
    "df.filter('(p21022 >= 50) AND (p31 = 0)').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python syntax\n",
    "df.filter((df.p21022 >= 50) & (df.p31 == 0)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting more information about fields\n",
    "\n",
    "Field objects (such as those returned by `fields_for_id` and `fields_by_title_keyword` above, or via the `participant[<field_name>]` syntax) provide additional information such as codings and units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Working with codings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "participant['p31'].coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "participant['p31'].coding.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def field_codes_by_keyword(field_name, keyword):\n",
    "    return dict([(k,v) for (k,v) in participant[field_name].coding.codes.items() if keyword.lower() in v.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "field_codes_by_keyword('p31', 'female')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "field_codes_by_keyword('p41202', 'obesity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional field information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get link to UKB documentation page\n",
    "participant['p31'].linkout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get field units\n",
    "participant['p21022'].units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as CSV file\n",
    "df.toPandas().to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as TSV file\n",
    "df.toPandas().to_csv('results.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as DTA file (Stata)\n",
    "df.toPandas().astype(str).replace('None|NaN|nan', '.', regex=True).to_stata('results.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing results back to the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "dx upload results.tsv --dest / "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
