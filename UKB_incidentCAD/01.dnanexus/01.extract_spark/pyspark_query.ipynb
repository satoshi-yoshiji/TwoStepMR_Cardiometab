{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook consists of three sections:\n",
    "### Step 1: Extract clinical phenotype data from dataset['participant'] \n",
    "### Step 2: Extract Olink proteomics from dataset['olink_instance_0']\n",
    "### Step 3: Extract operation records from dataset['hesin_oper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# dxpy allows python to interact with the platform storage\n",
    "# Note: This notebook is using spark since the size of the dataset we're extracting\n",
    "# (i.e. the number of fields) is too large for a single node instance.\n",
    "import dxpy\n",
    "import dxdata\n",
    "\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "# Spark initialization (Done only once; do not rerun this cell unless you select Kernel -> Restart kernel).\n",
    "# Need to adjust this buffer otherwise will get an error in toPandas() call\n",
    "conf = pyspark.SparkConf().set(\"spark.kryoserializer.buffer.max\", \"1024\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dxdata.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# silence warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Re-enable warnings after your code if you want to see warnings again in subsequent cells\n",
    "# warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically discover dispensed database name and dataset id\n",
    "dispensed_database = dxpy.find_one_data_object(\n",
    "    classname='database', \n",
    "    name='app*', \n",
    "    folder='/', \n",
    "    name_mode='glob', \n",
    "    describe=True)\n",
    "dispensed_database_name = dispensed_database['describe']['name']\n",
    "\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename='Dataset', \n",
    "    name='app*.dataset', \n",
    "    folder='/', \n",
    "    name_mode='glob')\n",
    "dispensed_dataset_id = dispensed_dataset['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 'entities' are virtual tables linked to one another.\n",
    "\n",
    "The main entity is 'participant' and corresponds to most pheno fields. Additional entities correspond to linked health care data.\n",
    "Entities starting with 'hesin' are for hospital records; entities starting with 'gp' are for GP records, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Accessing the main 'participant' entity\n",
    "The extraction code follows some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant = dataset['participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns all field objects for a given UKB showcase field id\n",
    "\n",
    "def fields_for_id(field_id):\n",
    "    from distutils.version import LooseVersion\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "# Returns all field names for a given UKB showcase field id\n",
    "\n",
    "def field_names_for_id(field_id):\n",
    "    return [f.name for f in fields_for_id(field_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain field name for all instances and arrays for each field_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk factors\n",
    "\"We defined risk factors at the first assessment as follows: diabetes diagnosed by a doctor (field #2443), BMI (field #21001), current smoking (field #20116), hypertension, family history of heart disease, and high cholesterol. For hypertension we used an expanded definition including self-reported high blood pressure (either on blood pressure medication, data fields #6177, #6153; systolic blood pressure >140 mm Hg, fields #4080, #93; or diastolic blood pressure >90 mm Hg, data fields #4079, #94). For family history of heart disease, we considered history in any first- degree relative (father, mother, sibling; fields #20107, 20110, and 20111, respectively). For high cholesterol, we considered individuals with self- reported high cholesterol at assessment, as well as diagnoses in the HES/death records (ICD-9 272.0; ICD-10 E78.0). For the analyses of the number of elevated risk factors, we considered diagnosed dia- betes (yes/no), hypertension at assessment (yes/no), BMI >30 kg/m2, smoking at assessment (yes/no), high cholesterol (yes/no), and family history of heart disease (yes/no).\"\n",
    "\n",
    " Am Coll Cardiol. 2018 Oct 16;72(16):1883-1893. \n",
    "https://pubmed.ncbi.nlm.nih.gov/30309464/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# risk factor + family history + plate id\n",
    "field_ids = ['2443', '20116', '6177', '6153', '4080', '93', '4079', '94', \n",
    "             '20107', '20110', '20111',\n",
    "             '30901']\n",
    "# sum flattens list of lists\n",
    "sum([field_names_for_id(field_id) for field_id in field_ids], []) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking up fields by title keyword\n",
    "\n",
    "If you remember part of the field title, use these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns all field objects for a given title keyword\n",
    "\n",
    "def fields_by_title_keyword(keyword):\n",
    "    from distutils.version import LooseVersion\n",
    "    fields = list(participant.find_fields(lambda f: keyword.lower() in f.title.lower()))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "# Returns all field names for a given title keyword\n",
    "\n",
    "def field_names_by_title_keyword(keyword):\n",
    "    return [f.name for f in fields_by_title_keyword(keyword)]\n",
    "\n",
    "# Returns all field titles for a given title keyword\n",
    "\n",
    "def field_titles_by_title_keyword(keyword):\n",
    "    return [f.title for f in fields_by_title_keyword(keyword)]\n",
    "\n",
    "# Furhter information: https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing fields into a Spark DataFrame\n",
    "\n",
    "The `participant.retrieve_fields` function can be used to construct a Spark DataFrame of the given fields.\n",
    "\n",
    "By default, this retrieves data as encoded by UK Biobank. For example, field p31 (participant sex) will be returned as an integer column with values of 0 and 1. To receive decoded values, supply the `coding_values='replace'` argument.\n",
    "\n",
    "For more information, see [Tips for Retrieving Fields](https://dnanexus.gitbook.io/uk-biobank-rap/getting-started/working-with-ukb-data#tips-for-retrieving-fields) in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract clinical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_names = ['eid', \n",
    "               'p31',  # sex\n",
    "               'p21022',  # age at recruitment\n",
    "               'p21001_i0',  # BMI\n",
    "               'p54_i0',  # UK Biobank assessment centre\n",
    "               'p53_i0',  # Date of attending assessment centre p53_i0\n",
    "               'p40000_i0',  # Date of death\n",
    "               'p191', # date lost to follow-up\n",
    "               'p20003_i0_a0', # medication\n",
    "               'p6150_i0',  # having had a heart attack diagnosed by a doctor \n",
    "               'p20002_i0_a0',  # non-cancer illnesses that self-reported as heart attack \n",
    "               'p20004_i0_a0',  # self-reported operation including PTCA, CABG, or triple heart bypass\n",
    "               'p2443_i0',  # diabetes\n",
    "               'p20116_i0',  # current smoking\n",
    "               'p20160_i0', # ever smoked\n",
    "               'p6177_i0',  # self-reported high blood pressure\n",
    "               'p6153_i0',  # self-reported high blood pressure\n",
    "               'p4080_i0_a0',  # systolic blood pressure \n",
    "               'p93_i0_a0',  # systolic blood pressure \n",
    "               'p4079_i0_a0',  # diastolic blood pressure\n",
    "               'p94_i0_a0', # diastolic blood pressure\n",
    "               'p6177_i0', # Medication for cholesterol, blood pressure or diabetes | Instance 0\n",
    "               'p20107_i0', #illness of father\n",
    "               'p20110_i1', # illness of mother\n",
    "               'p20111_i0', # illness of sibling\n",
    "               'p30901_i0', # olink plateID\n",
    "              ] \\\n",
    "                + field_names_for_id('41270') \\\n",
    "                + field_names_for_id('41280')  # Corrected line\n",
    "\n",
    "# 41270 = ICD10; 41280 = Date of first in-patient diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Further informatiaon on Date of first in-patient diagnosis can be found at https://biobank.ndph.ox.ac.uk/crystal/field.cgi?id=41280:\n",
    "The corresponding ICD-10 diagnosis codes can be found in data-field Field 41270 and the two fields can be linked using the array structure.\n",
    "\n",
    "• CAD definition and risk factors:\n",
    "J Am Coll Cardiol. 2018 Oct 16;72(16):1883-1893. \n",
    "https://pubmed.ncbi.nlm.nih.gov/30309464/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing fields into a Spark DataFrame\n",
    "The participant.retrieve_fields function can be used to construct a Spark DataFrame of the given fields.\n",
    "\n",
    "By default, this retrieves data as encoded by UK Biobank. For example, field p31 (participant sex) will be returned as an integer column with values of 0 and 1. To receive decoded values, supply the coding_values='replace' argument.\n",
    "\n",
    "For more information, see Tips for Retrieving Fields in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grabbing fields into a Spark DataFrame\n",
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See the first five entries as a Pandas DataFrame:\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if the above looks good, go ahead and convert the entire spark data frame to pandas data frame \n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving as TSV file\n",
    "pdf.to_csv('clinical_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Olink proteomics from dataset['olink_instance_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "olink = dataset['olink_instance_0']\n",
    "# olink.fields # to list all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_list = olink.fields\n",
    "type(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def field_names_for_id(field_id):\n",
    "#    return [f.name for f in fields_for_id(field_id)]\n",
    "olink_all_field_names = [f.name for f in olink.fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#field_names = ['eid', 'col6a3'] # select like this if you're interested in only a few protein\n",
    "olink_all_field_names[:5] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfo = olink.retrieve_fields(names=olink_all_field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to check\n",
    "# dfo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case of extracting particular proteins only:\n",
    "olink_field_names = ['eid', 'col6a3']\n",
    "dfo = olink.retrieve_fields(names=olink_field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See the first five entries as a Pandas DataFrame:\n",
    "# dfo.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfo.count() # check rows (53016 individual's data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo = dfo.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo.shape # (53016, 2924)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative approach\n",
    "# pdfo = dfo.toPandas() is very memory intensive. So instead, we can do this sequentially (if needed).\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = dfo.count()\n",
    "\n",
    "# Number of chunks\n",
    "num_chunks = 10\n",
    "\n",
    "# Calculate the number of rows per chunk. Adding 1 to ensure the last chunk includes all remaining rows\n",
    "rows_per_chunk = (total_rows // num_chunks) + (total_rows % num_chunks > 0)\n",
    "\n",
    "# Initialize an empty list to store each chunk's pandas DataFrame\n",
    "chunks_list = []\n",
    "\n",
    "# Create a column 'row_id' to help in filtering rows for each chunk\n",
    "dfo_with_id = dfo.withColumn(\"row_id\", row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start index for the current chunk\n",
    "    start_index = i * rows_per_chunk\n",
    "    \n",
    "    # End index is not needed as we limit the number of rows fetched\n",
    "    chunk_df = dfo_with_id.filter(dfo_with_id.row_id >= start_index).limit(rows_per_chunk)\n",
    "    \n",
    "    # Convert the chunk to a pandas DataFrame and append to the list\n",
    "    chunk_pd_df = chunk_df.drop(\"row_id\").toPandas()\n",
    "    chunks_list.append(chunk_pd_df)\n",
    "\n",
    "# Concatenate all chunks to form the final pandas DataFrame\n",
    "pdf = pd.concat(chunks_list, ignore_index=True)\n",
    "\n",
    "# Checking the shape of the final DataFrame\n",
    "print(pdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo.iloc[:5, :5] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo.to_csv('olink_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Extract operation records from dataset['hesin_oper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operation = dataset['hesin_oper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# operation.fields[1:5] # check\n",
    "operation.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operation_all_field_names = [f.name for f in operation.fields]\n",
    "print(operation_all_field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ope = operation.retrieve_fields(names=operation_all_field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ope.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ope.limit(5).toPandas().head() # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternatively, we do this sequentially\n",
    "# from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = df_ope.count()\n",
    "\n",
    "# Number of chunks\n",
    "num_chunks = 10\n",
    "\n",
    "# Calculate the number of rows per chunk. Adding 1 to ensure the last chunk includes all remaining rows\n",
    "rows_per_chunk = (total_rows // num_chunks) + (total_rows % num_chunks > 0)\n",
    "\n",
    "# Initialize an empty list to store each chunk's pandas DataFrame\n",
    "chunks_list = []\n",
    "\n",
    "# Create a column 'row_id' to help in filtering rows for each chunk\n",
    "df_ope_with_id = df_ope.withColumn(\"row_id\", row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start index for the current chunk\n",
    "    start_index = i * rows_per_chunk\n",
    "    \n",
    "    # End index is not needed as we limit the number of rows fetched\n",
    "    chunk_df = df_ope_with_id.filter(df_ope_with_id.row_id >= start_index).limit(rows_per_chunk)\n",
    "    \n",
    "    # Convert the chunk to a pandas DataFrame and append to the list\n",
    "    chunk_pd_df = chunk_df.drop(\"row_id\").toPandas()\n",
    "    chunks_list.append(chunk_pd_df)\n",
    "\n",
    "# Concatenate all chunks to form the final pandas DataFrame\n",
    "pdf_ope = pd.concat(chunks_list, ignore_index=True)\n",
    "\n",
    "# Checking the shape of the final DataFrame\n",
    "print(pdf_ope.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_ope.to_csv('operation_data.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hesin is optional\n",
    "# hesin = dataset['hesin']\n",
    "# hesin_all_field_names = [f.name for f in hesin.fields]\n",
    "# print(hesin_all_field_names)\n",
    "# df_hesin = hesin.retrieve_fields(names=hesin_all_field_names, engine=dxdata.connect())\n",
    "# df_hesin.limit(5).toPandas().head() # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrive hesin_diag, which contains ICD10\n",
    "hesin_diag = dataset['hesin_diag']\n",
    "hesin_diag_all_field_names = [f.name for f in hesin_diag.fields]\n",
    "print(hesin_diag_all_field_names)\n",
    "df_hesin_diag = hesin_diag.retrieve_fields(names=hesin_diag_all_field_names, engine=dxdata.connect())\n",
    "df_hesin_diag.limit(5).toPandas().head() # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# again, toPandas() is too memory intensive\n",
    "# pdf_hesin_diag = df_hesin_diag.dropna(subset = ['diag_icd10']).toPandas()\n",
    "# Alternatively, we do this sequentially\n",
    "# from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = df_hesin_diag.count()\n",
    "\n",
    "# Number of chunks\n",
    "num_chunks = 10\n",
    "\n",
    "# Calculate the number of rows per chunk. Adding 1 to ensure the last chunk includes all remaining rows\n",
    "rows_per_chunk = (total_rows // num_chunks) + (total_rows % num_chunks > 0)\n",
    "\n",
    "# Initialize an empty list to store each chunk's pandas DataFrame\n",
    "chunks_list = []\n",
    "\n",
    "# Create a column 'row_id' to help in filtering rows for each chunk\n",
    "df_hesin_diag_with_id = df_hesin_diag.withColumn(\"row_id\", row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start index for the current chunk\n",
    "    start_index = i * rows_per_chunk\n",
    "    \n",
    "    # End index is not needed as we limit the number of rows fetched\n",
    "    chunk_df = df_hesin_diag_with_id.filter(df_hesin_diag_with_id.row_id >= start_index).limit(rows_per_chunk)\n",
    "    \n",
    "    # Convert the chunk to a pandas DataFrame and append to the list\n",
    "    chunk_pd_df = chunk_df.drop(\"row_id\").toPandas()\n",
    "    chunks_list.append(chunk_pd_df)\n",
    "\n",
    "# Concatenate all chunks to form the final pandas DataFrame\n",
    "pdf_hesin_diag = pd.concat(chunks_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pdf_hesin_diag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_hesin_diag.to_csv('operation_icd10_data.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge df_ope and df_hesin_diag (this is too memory intensive)\n",
    "# pdf_ope_diag = pd.merge(pdf_ope, pdf_hesin_diag, on='eid', how='inner')\n",
    "# pdf_ope_diag.head() # check\n",
    "# pdf_ope_diag.to_csv('operation_data_with_icd10.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# retrive death record\n",
    "hesin = dataset['hesin']\n",
    "hesin_all_field_names = [f.name for f in hesin.fields]\n",
    "print(hesin_all_field_names)\n",
    "df_hesin = hesin.retrieve_fields(names=hesin_all_field_names, engine=dxdata.connect())\n",
    "df_hesin.limit(5).toPandas().head() # check#### Step 4: Extract death record\n",
    "4.1. <Entity \"death\">,\n",
    "4.2 <Entity \"death_cause\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death = dataset['death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death_all_field_names = [f.name for f in death.fields]\n",
    "print(death_all_field_names)\n",
    "df_death = death.retrieve_fields(names=death_all_field_names, engine=dxdata.connect())\n",
    "df_death.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_death = df_death.toPandas() # convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_death.to_csv('death_data.tsv', sep= '\\t', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4.2\n",
    "death_cause = dataset['death_cause']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(death_cause.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death_cause_all_field_names = [f.name for f in death_cause.fields]\n",
    "print(death_cause_all_field_names)\n",
    "df_death_cause = death_cause.retrieve_fields(names=death_cause_all_field_names, engine=dxdata.connect())\n",
    "df_death_cause.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_death_cause = df_death_cause.toPandas() # convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_death_cause.to_csv('death_cause_data.tsv', sep= '\\t', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload the resultant files and the current notebook (after saving it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx upload clinical_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload olink_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload operation_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload operation_icd10_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload death_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload death_cause_data.tsv --dest UKB:/data/03.incident_CAD/\n",
    "dx upload ukbrap_extract_clinical_olink_ope_data.ipynb --dest UKB:/data/03.incident_CAD/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
