{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pQTL relevant data extraction using PySpark on UKB RAP\n",
    "This notebook is built upon two notebooks from DNAnexus:\n",
    "https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb\n",
    "https://github.com/dnanexus/UKB_RAP/blob/main/proteomics/0_extract_phenotype_protein_data.ipynb\n",
    "\n",
    "### This notebook consists of three sections:\n",
    "### Step 1: Extract clinical phenotype data from dataset['participant'] \n",
    "### Step 2: Extract Olink proteomics from dataset['olink_instance_0']\n",
    "### Step 3: Extract operation records from dataset['hesin_oper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# dxpy allows python to interact with the platform storage\n",
    "# Note: This notebook is using spark since the size of the dataset we're extracting\n",
    "# (i.e. the number of fields) is too large for a single node instance.\n",
    "import dxpy\n",
    "import dxdata\n",
    "\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "# Spark initialization (Done only once; do not rerun this cell unless you select Kernel -> Restart kernel).\n",
    "# Need to adjust this buffer otherwise will get an error in toPandas() call\n",
    "conf = pyspark.SparkConf().set(\"spark.kryoserializer.buffer.max\", \"1024\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normal spark initialization (Done only once; do not rerun this cell unless you select Kernel -> Restart kernel).\n",
    "# sc = pyspark.SparkContext()\n",
    "# spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dxdata.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# silence warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Re-enable warnings after your code if you want to see warnings again in subsequent cells\n",
    "# warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the option to None to display all rows, or set a specific number\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Reset display options to default\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically discover dispensed database name and dataset id\n",
    "dispensed_database = dxpy.find_one_data_object(\n",
    "    classname='database', \n",
    "    name='app*', \n",
    "    folder='/', \n",
    "    name_mode='glob', \n",
    "    describe=True)\n",
    "dispensed_database_name = dispensed_database['describe']['name']\n",
    "\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename='Dataset', \n",
    "    name='app*.dataset', \n",
    "    folder='/', \n",
    "    name_mode='glob')\n",
    "dispensed_dataset_id = dispensed_dataset['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 'entities' are virtual tables linked to one another.\n",
    "\n",
    "The main entity is 'participant' and corresponds to most pheno fields. Additional entities correspond to linked health care data.\n",
    "Entities starting with 'hesin' are for hospital records; entities starting with 'gp' are for GP records, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Accessing the main 'participant' entity\n",
    "The extraction code follows some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant = dataset['participant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing field names, given UKB showcase field id, instance id, and array id\n",
    "\n",
    "For the main participant data, the Platform uses field names with the following convention:\n",
    "\n",
    "|Type of field|Syntax for field name|Example|\n",
    "|:------------|---------------------|-------|\n",
    "|Neither instanced nor arrayed|`p<FIELD-ID>`|`p31`|\n",
    "|Instanced but not arrayed|`p<FIELD-ID>_i<INSTANCE-ID>`|`p40005_i0`|\n",
    "|Arrayed but not instanced|`p<FIELD-ID>_a<ARRAY-ID>`|`p41262_a0`|\n",
    "|Instanced and arrayed|`p<FIELD-ID>_i<INSTANCE-ID>_a<ARRAY-ID>`|`p93_i0_a0`|\n",
    "\n",
    "Lastly, the participant id field itself (EID) is named `eid`\n",
    "\n",
    "If you know exactly the field names you want to work with, put them in a string array (we will see later how to use that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example\n",
    "# field_names = ['eid', 'p31', 'p21022', 'p40005_i0', 'p93_i0_a0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking up fields, given UKB showcase field id\n",
    "\n",
    "If you know the field id but you are not sure if it is instanced or arrayed, and want to grab all instances/arrays (if any), use these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns all field objects for a given UKB showcase field id\n",
    "\n",
    "def fields_for_id(field_id):\n",
    "    from distutils.version import LooseVersion\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "# Returns all field names for a given UKB showcase field id\n",
    "\n",
    "def field_names_for_id(field_id):\n",
    "    return [f.name for f in fields_for_id(field_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Participant sex\n",
    "field_names_for_id('31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Age when attending assessment centre has multiple instances (visits) \n",
    "field_names_for_id('21003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_names_for_id('21001') # BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_names_for_id('22009') # PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking up fields by title keyword\n",
    "\n",
    "If you remember part of the field title, use these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns all field objects for a given title keyword\n",
    "\n",
    "def fields_by_title_keyword(keyword):\n",
    "    from distutils.version import LooseVersion\n",
    "    fields = list(participant.find_fields(lambda f: keyword.lower() in f.title.lower()))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "# Returns all field names for a given title keyword\n",
    "\n",
    "def field_names_by_title_keyword(keyword):\n",
    "    return [f.name for f in fields_by_title_keyword(keyword)]\n",
    "\n",
    "# Returns all field titles for a given title keyword\n",
    "\n",
    "def field_titles_by_title_keyword(keyword):\n",
    "    return [f.title for f in fields_by_title_keyword(keyword)]\n",
    "\n",
    "# Furhter information: https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing fields into a Spark DataFrame\n",
    "\n",
    "The `participant.retrieve_fields` function can be used to construct a Spark DataFrame of the given fields.\n",
    "\n",
    "By default, this retrieves data as encoded by UK Biobank. For example, field p31 (participant sex) will be returned as an integer column with values of 0 and 1. To receive decoded values, supply the `coding_values='replace'` argument.\n",
    "\n",
    "For more information, see [Tips for Retrieving Fields](https://dnanexus.gitbook.io/uk-biobank-rap/getting-started/working-with-ukb-data#tips-for-retrieving-fields) in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract clinical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_names = ['eid', \n",
    "               'p31',  # sex\n",
    "               'p21022',  # age at recruitment\n",
    "               'p21001_i0',  # BMI\n",
    "               'p54_i0',  # UK Biobank assessment centre\n",
    "               'p53_i0',  # Date of attending assessment centre p53_i0\n",
    "               'p3166_i0_a0', # time of blood collection\n",
    "               'p22019', # Sex_chromosome_aneuploidy\n",
    "               'p22000', # genotype batch\n",
    "               'p22006',# genetic grouping, \n",
    "               'p30901_i0', # olink plateID\n",
    "              ] \\\n",
    "                + field_names_for_id('22009') \n",
    "\n",
    "# 41270 = ICD10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Further informatiaon on Date of first in-patient diagnosis can be found at https://biobank.ndph.ox.ac.uk/crystal/field.cgi?id=41280:\n",
    "The corresponding ICD-10 diagnosis codes can be found in data-field Field 41270 and the two fields can be linked using the array structure.\n",
    "\n",
    "• CAD definition and risk factors:\n",
    "J Am Coll Cardiol. 2018 Oct 16;72(16):1883-1893. \n",
    "https://pubmed.ncbi.nlm.nih.gov/30309464/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing fields into a Spark DataFrame\n",
    "The participant.retrieve_fields function can be used to construct a Spark DataFrame of the given fields.\n",
    "\n",
    "By default, this retrieves data as encoded by UK Biobank. For example, field p31 (participant sex) will be returned as an integer column with values of 0 and 1. To receive decoded values, supply the coding_values='replace' argument.\n",
    "\n",
    "For more information, see Tips for Retrieving Fields in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grabbing fields into a Spark DataFrame\n",
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See the first five entries as a Spark DataFrame:\n",
    "# df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See the first five entries as a Pandas DataFrame:\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if the above looks good, go ahead and convert the entire spark data frame to pandas data frame \n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the option to None to display all rows, or set a specific number\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Your code to display the Series or DataFrame\n",
    "# For example, if you have a Series named non_na_counts:\n",
    "# pdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Given the information above, re-select the column if needed\n",
    "#field_names = ['p31', 'p21022', 'p54_i0', 'p41202', 'p53_i0', 'p40000_i0']  \\\n",
    "#    + field_names_for_id('41280') \n",
    "#df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset display options to default\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving as TSV file\n",
    "pdf.to_csv('clinical_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Olink proteomics from dataset['olink_instance_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "olink = dataset['olink_instance_0']\n",
    "# olink.fields # to list all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_list = olink.fields\n",
    "type(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def field_names_for_id(field_id):\n",
    "#    return [f.name for f in fields_for_id(field_id)]\n",
    "olink_all_field_names = [f.name for f in olink.fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#field_names = ['eid', 'col6a3'] # select like this if you're interested in only a few protein\n",
    "olink_all_field_names[:5] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfo = olink.retrieve_fields(names=olink_all_field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to check\n",
    "# dfo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case of extracting particular proteins only:\n",
    "olink_field_names = ['eid', 'col6a3']\n",
    "dfo = olink.retrieve_fields(names=olink_field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See the first five entries as a Pandas DataFrame:\n",
    "# dfo.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfo.count() # check rows (53016 individual's data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo = dfo.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo.shape # (53016, 2924)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative approach\n",
    "# pdfo = dfo.toPandas() is very memory intensive. So instead, we can do this sequentially (if needed).\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = dfo.count()\n",
    "\n",
    "# Number of chunks\n",
    "num_chunks = 10\n",
    "\n",
    "# Calculate the number of rows per chunk. Adding 1 to ensure the last chunk includes all remaining rows\n",
    "rows_per_chunk = (total_rows // num_chunks) + (total_rows % num_chunks > 0)\n",
    "\n",
    "# Initialize an empty list to store each chunk's pandas DataFrame\n",
    "chunks_list = []\n",
    "\n",
    "# Create a column 'row_id' to help in filtering rows for each chunk\n",
    "dfo_with_id = dfo.withColumn(\"row_id\", row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start index for the current chunk\n",
    "    start_index = i * rows_per_chunk\n",
    "    \n",
    "    # End index is not needed as we limit the number of rows fetched\n",
    "    chunk_df = dfo_with_id.filter(dfo_with_id.row_id >= start_index).limit(rows_per_chunk)\n",
    "    \n",
    "    # Convert the chunk to a pandas DataFrame and append to the list\n",
    "    chunk_pd_df = chunk_df.drop(\"row_id\").toPandas()\n",
    "    chunks_list.append(chunk_pd_df)\n",
    "\n",
    "# Concatenate all chunks to form the final pandas DataFrame\n",
    "pdf = pd.concat(chunks_list, ignore_index=True)\n",
    "\n",
    "# Checking the shape of the final DataFrame\n",
    "print(pdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo.iloc[:5, :5] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfo.to_csv('olink_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                          file-GjFF0zjJBPpv7b56626G5vG4\n",
      "Class                       file\n",
      "Project                     project-Gf7Jk4QJBPpxxxG2x187pyJg\n",
      "Folder                      /data/04.COL6A3\n",
      "Name                        ukbrap_extract_clinical_olink_data.ipynb\n",
      "State                       closing\n",
      "Visibility                  visible\n",
      "Types                       -\n",
      "Properties                  -\n",
      "Tags                        -\n",
      "Outgoing links              -\n",
      "Created                     Wed Apr 10 15:30:07 2024\n",
      "Created by                  satoshi.yoshiji\n",
      " via the job                job-GjF9jx0JBPpbPqBx1Fy7zKqY\n",
      "Last modified               Wed Apr 10 15:30:08 2024\n",
      "Media type                  \n",
      "archivalState               \"live\"\n",
      "cloudAccount                \"cloudaccount-dnanexus\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#dx upload clinical_data.tsv --dest UKB:/data/04.COL6A3/\n",
    "#dx upload olink_data.tsv --dest UKB:/data/04.COL6A3/\n",
    "dx upload ukbrap_extract_clinical_olink_data.ipynb --dest UKB:/data/04.COL6A3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Below are miscellaneous codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if you want to combine data frame, use pandas's merge \n",
    "combined = pd.merge(pdf, df_final, on = 'eid', how= 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering spark dataframes\n",
    "\n",
    "Spark dataframes can be filtered using the syntax: `df.filter(expression)`\n",
    "\n",
    "The expression can be either :\n",
    "\n",
    "* a string expression, built using SQL field names (e.g. `p31`) and SQL operators (e.g. `=`, `NOT`, `OR`, `AND`)\n",
    "  * example: `'(p21022 >= 50) AND (p31 = 0)'`\n",
    "  \n",
    "\n",
    "* a Python expression, built using Python object fields (e.g. `df.p31`) and Python operators (e.g. `==`, `!`, `|`, `&`)\n",
    "  * example: `(df.p21022 >= 50) & (df.p31 == 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Participants above 50 years old and female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL syntax\n",
    "df.filter('(p21022 >= 50) AND (p31 = 0)').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python syntax\n",
    "df.filter((df.p21022 >= 50) & (df.p31 == 0)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting more information about fields\n",
    "\n",
    "Field objects (such as those returned by `fields_for_id` and `fields_by_title_keyword` above, or via the `participant[<field_name>]` syntax) provide additional information such as codings and units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Working with codings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "participant['p31'].coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "participant['p31'].coding.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def field_codes_by_keyword(field_name, keyword):\n",
    "    return dict([(k,v) for (k,v) in participant[field_name].coding.codes.items() if keyword.lower() in v.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "field_codes_by_keyword('p31', 'female')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "field_codes_by_keyword('p41202', 'obesity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional field information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get link to UKB documentation page\n",
    "participant['p31'].linkout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get field units\n",
    "participant['p21022'].units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as CSV file\n",
    "df.toPandas().to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as TSV file\n",
    "df.toPandas().to_csv('results.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as DTA file (Stata)\n",
    "df.toPandas().astype(str).replace('None|NaN|nan', '.', regex=True).to_stata('results.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing results back to the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "dx upload results.tsv --dest / "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
